{
 "metadata": {
  "name": "",
  "signature": "sha256:f15351908cc96038607d6333a4885307db9d1e96e2f87989b2940317f6e0b34d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import nltk\n",
      "import math\n",
      "\n",
      "#this function takes the words from the training data and returns a python list of all of the words that occur more than 5 times\n",
      "#wbrown is a python list where every element is a python list of the words of a particular sentence\n",
      "def calc_known(wbrown):\n",
      "    knownwords = []\n",
      "    dic={}\n",
      "    for l in wbrown:\n",
      "        for w in l:\n",
      "            if w in dic:\n",
      "                dic[w] += 1\n",
      "            else:\n",
      "                dic[w]=1\n",
      "            if dic[w]==5:\n",
      "                knownwords.append(w)\n",
      "    return knownwords\n",
      "\n",
      "#this function takes a set of sentences and a set of words that should not be marked '_RARE_'\n",
      "#brown is a python list where every element is a python list of the words of a particular sentence\n",
      "#and outputs a version of the set of sentences with rare words marked '_RARE_'\n",
      "def replace_rare(brown, knownwords):\n",
      "    rare = []\n",
      "    for l in brown:\n",
      "        temp=[]\n",
      "        for w in l:\n",
      "            if w in knownwords:\n",
      "                temp.append(w)\n",
      "            else:\n",
      "                temp.append(\"_RARE_\")\n",
      "        rare.append(temp) \n",
      "    return rare\n",
      "\n",
      "#this function takes the ouput from replace_rare and outputs it\n",
      "def q3_output(rare):\n",
      "    outfile = open(\"B3.txt\", 'w')\n",
      "\n",
      "    for sentence in rare:\n",
      "        outfile.write(' '.join(sentence[2:-1]) + '\\n')\n",
      "    outfile.close()\n",
      "\n",
      "#this function takes tags from the training data and calculates trigram probabilities\n",
      "#tbrown (the list of tags) should be a python list where every element is a python list of the tags of a particular sentence\n",
      "#it returns a python dictionary where the keys are tuples that represent the trigram, and the values are the log probability of that trigram\n",
      "def calc_trigrams(tbrown):\n",
      "    qvalues = {}\n",
      "    c={}\n",
      "    for l in tbrown:\n",
      "        for i in range(2, len(l)):\n",
      "            key=tuple((l[i-2], l[i-1], l[i],))\n",
      "            if key in qvalues:\n",
      "                qvalues[key] +=1\n",
      "            else:\n",
      "                qvalues[key] = 1\n",
      "                \n",
      "            key=tuple((l[i-1], l[i],))\n",
      "            if key in c:\n",
      "                c[key] +=1\n",
      "            else:\n",
      "                c[key] = 1\n",
      "    c[('*','*')]=len(tbrown)\n",
      "    for key in qvalues:\n",
      "        qvalues[key] = math.log(1.0*qvalues[key]/c[key[:-1]], 2)\n",
      "    \n",
      "    return qvalues\n",
      "\n",
      "#this function takes output from calc_trigrams() and outputs it in the proper format\n",
      "def q2_output(qvalues):\n",
      "    #output \n",
      "    outfile = open(\"B2.txt\", \"w\")\n",
      "    for trigram in qvalues:\n",
      "        output = \" \".join(['TRIGRAM', trigram[0], trigram[1], trigram[2], str(qvalues[trigram])])\n",
      "        outfile.write(output + '\\n')\n",
      "    outfile.close()\n",
      "\n",
      "#this function calculates emission probabilities and creates a list of possible tags\n",
      "#the first return value is a python dictionary where each key is a tuple in which the first element is a word \n",
      "#and the second is a tag and the value is the log probability of that word/tag pair\n",
      "#and the second return value is a list of possible tags for this data set\n",
      "#wbrown is a python list where each element is a python list of the words of a particular sentence\n",
      "#tbrown is a python list where each element is a python list of the tags of a particular sentence\n",
      "def calc_emission(wbrown, tbrown):\n",
      "    evalues = {}\n",
      "    taglist = []\n",
      "    return evalues, taglist\n",
      "\n",
      "#this function takes the output from calc_emissions() and outputs it\n",
      "def q4_output(evalues):\n",
      "    #output\n",
      "    outfile = open(\"B4.txt\", \"w\")\n",
      "    for item in evalues:\n",
      "        output = \" \".join([item[0], item[1], str(evalues[item])])\n",
      "        outfile.write(output + '\\n')\n",
      "    outfile.close()\n",
      "\n",
      "\n",
      "#this function takes data to tag (brown), possible tags (taglist), a list of known words (knownwords), \n",
      "#trigram probabilities (qvalues) and emission probabilities (evalues) and outputs a list where every element is a string of a \n",
      "#sentence tagged in the WORD/TAG format \n",
      "#brown is a list where every element is a list of words\n",
      "#taglist is from the return of calc_emissions()\n",
      "#knownwords is from the the return of calc_knownwords()\n",
      "#qvalues is from the return of calc_trigrams\n",
      "#evalues is from the return of calc_emissions() \n",
      "#tagged is a list of tagged sentences in the format \"WORD/TAG\". Each sentence is a string, not a list of tokens.\n",
      "def viterbi(brown, taglist, knownwords, qvalues, evalues):\n",
      "    tagged = []\n",
      "    return tagged\n",
      "\n",
      "#this function takes the output of viterbi() and outputs it\n",
      "def q5_output(tagged):\n",
      "    outfile = open('B5.txt', 'w')\n",
      "    for sentence in tagged:\n",
      "        outfile.write(sentence)\n",
      "    outfile.close()\n",
      "\n",
      "#this function uses nltk to create the taggers described in question 6\n",
      "#brown is the data to be tagged\n",
      "#tagged is a list of tagged sentences. Each sentence is in the WORD/TAG format and is a string rather than a list of tokens.\n",
      "def nltk_tagger(brown):\n",
      "    tagged = []\n",
      "    return tagged\n",
      "\n",
      "def q6_output(tagged):\n",
      "    outfile = open('B4.txt', 'w')\n",
      "\n",
      "    for sentence in tagged:\n",
      "        outfile.write(sentence)\n",
      "    outfile.close()\n",
      "\n",
      "#a function that returns two lists, one of the brown data (words only) and another of the brown data (tags only) \n",
      "def split_wordtags(brown_train):\n",
      "    \n",
      "    def split_dash(word):\n",
      "        i=len(word)\n",
      "        while word[i-1]!='/' and i>0:\n",
      "            i=i-1\n",
      "        return word[:(i-1)], word[i:]\n",
      "    \n",
      "    wbrown = []\n",
      "    tbrown = []\n",
      "    for l in brown_train:\n",
      "        wl=[]\n",
      "        tl=[]\n",
      "        tokens = l.split() #nltk.word_tokenize(l)\n",
      "        wl += [\"*\",  \"*\"]\n",
      "        tl += [\"*\",  \"*\"]\n",
      "        for e in tokens:\n",
      "            s=split_dash(e)\n",
      "            wl.append(s[0])\n",
      "            tl.append(s[1])\n",
      "        wl.append(\"STOP\")\n",
      "        tl.append(\"STOP\")\n",
      "        wbrown+=[wl]\n",
      "        tbrown+=[tl]\n",
      "    return wbrown, tbrown\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "    #open Brown training data\n",
      "    infile = open(\"Brown_tagged_train.txt\", \"r\")\n",
      "    brown_train = infile.readlines()\n",
      "    infile.close()\n",
      "    brown_train=brown_train[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "    #split words and tags, and add start and stop symbols (question 1)\n",
      "    wbrown, tbrown = split_wordtags(brown_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 102
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "           \n",
      "    #calculate trigram probabilities (question 2)\n",
      "qvalues = calc_trigrams(tbrown)\n",
      "    \n",
      "    #question 2 output\n",
      "q2_output(qvalues)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 103
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "    #calculate list of words with count > 5 (question 3)\n",
      "    knownwords = calc_known(wbrown)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "    #get a version of wbrown with rare words replace with '_RARE_' (question 3)\n",
      "    wbrown_rare = replace_rare(wbrown, knownwords)\n",
      "\n",
      "    #question 3 output\n",
      "    q3_output(wbrown_rare)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 105
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wbrown_rare\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 106,
       "text": [
        "[['*',\n",
        "  '*',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  'to',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '.',\n",
        "  'STOP'],\n",
        " ['*',\n",
        "  '*',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  'the',\n",
        "  '_RARE_',\n",
        "  'of',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '.',\n",
        "  'STOP'],\n",
        " ['*',\n",
        "  '*',\n",
        "  '_RARE_',\n",
        "  'the',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  'the',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '.',\n",
        "  'STOP'],\n",
        " ['*',\n",
        "  '*',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  'to',\n",
        "  'the',\n",
        "  '_RARE_',\n",
        "  '.',\n",
        "  'STOP'],\n",
        " ['*',\n",
        "  '*',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  'the',\n",
        "  '_RARE_',\n",
        "  'of',\n",
        "  '_RARE_',\n",
        "  'of',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '.',\n",
        "  'STOP'],\n",
        " ['*',\n",
        "  '*',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  'of',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '.',\n",
        "  'STOP'],\n",
        " ['*',\n",
        "  '*',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '.',\n",
        "  'STOP'],\n",
        " ['*',\n",
        "  '*',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  'the',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  'the',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  'to',\n",
        "  'the',\n",
        "  '_RARE_',\n",
        "  'of',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '.',\n",
        "  'STOP'],\n",
        " ['*',\n",
        "  '*',\n",
        "  '_RARE_',\n",
        "  'the',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  'the',\n",
        "  '_RARE_',\n",
        "  'of',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  'to',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  'to',\n",
        "  '_RARE_',\n",
        "  'to',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '.',\n",
        "  'STOP'],\n",
        " ['*',\n",
        "  '*',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '_RARE_',\n",
        "  '.',\n",
        "  'STOP']]"
       ]
      }
     ],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "    #calculate emission probabilities (question 4)\n",
      "    evalues, taglist = calc_emission(wbrown_rare, tbrown)\n",
      "\n",
      "    #question 4 output\n",
      "    q4_output(evalues)\n",
      "\n",
      "    #delete unneceessary data\n",
      "    del brown_train\n",
      "    del wbrown\n",
      "    del tbrown\n",
      "    del wbrown_rare\n",
      "\n",
      "    #open Brown development data (question 5)\n",
      "    infile = open(\"Brown_dev.txt\", \"r\")\n",
      "    brown_dev = infile.readlines()\n",
      "    infile.close()\n",
      "\n",
      "\n",
      "    #replace rare words in brown_dev (question 5)\n",
      "    brown_dev_rare = replace_rare(brown_dev, knownwords)\n",
      "\n",
      "    #do viterbi on brown_dev (question 5)\n",
      "    viterbi_tagged = viterbi(brown_dev, taglist, knownwords, qvalues, evalues)\n",
      "\n",
      "    #question 5 output\n",
      "    q5_output(viterbi_tagged)\n",
      "\n",
      "    #do nltk tagging here\n",
      "    nltk_tagged = nltk_tagger(brown_dev)\n",
      "    \n",
      "    #question 6 output\n",
      "    q6_output(nltk_tagged)\n",
      "if __name__ == \"__main__\": main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}